{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai wikipedia-api tiktoken transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI KEY lesen\n",
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_KEY = userdata.get('OPENAI_KEY')\n",
    "except:\n",
    "    OPENAI_KEY = os.getenv('OPENAI_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beispiel-Dokumente von Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import wikipediaapi\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_name = 'Elvis_Presley'\n",
    "\n",
    "wiki = wikipediaapi.Wikipedia('LangChain RAG', 'de', extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "text = wiki.page(page_name).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übersetzen\n",
    "\n",
    "Als Beispiel wollen wir den `Elvis` Artikel übersetzen. Herausforderung dabei? Der Artikel ist sehr lang und wir bekommen Probleme mit der Kontext-Länge von `GPT-3.5 Turbo`. Es erscheint eine Fehlermeldung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''{text}\n",
    "Übersetze den folgenden Text ins englische:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens zählen\n",
    "\n",
    "siehe auch den Tokenizer auf der OpenAI Website: https://platform.openai.com/tokenizer\n",
    "\n",
    "Um das Problem zu umgehen kürzen wir den Artikel - dazu zählen wir mit `tiktoken` die Anzahl der Tokens des Textes und schneiden ihn nach 15.000 Tokens ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wir schneiden den text nach n-Tokens ab\n",
    "n = 15000\n",
    "tokenized_text = encoding.encode(text)\n",
    "tokenized_text = tokenized_text[:n]\n",
    "\n",
    "tokenized_text[:10], text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.decode([6719])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir den gekürzten Text (sind jetzt ja Tokens anstatt lesbarer Text) wieder in den Prompt \"stecken\" können, wandeln wir ihn wieder in Text um. Gekürzt sollte jetzt auch keine Fehlermeldung mehr erscheinen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = encoding.decode(tokenized_text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''{text}\n",
    "\n",
    "Übersetze den Text vollständig ins englische:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=None\n",
    "    )\n",
    "\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoding.encode(completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"finish_reason\"\n",
    "\n",
    "In der Ausgabe der OpenAI API bekommen wir das Attribut \"finish_reason\" zurück. Das gibt an wieso die Ausgabe beendet wurde. \n",
    "- `stop` bedeutet, dass das Modell fertig ist und nicht mehr Text erzeugen möchte\n",
    "- `length` bedeutet, dass die Kontext-Länge beim erzeugen des Textes überschritten wurde und der Text abgeschnitten ist\n",
    "\n",
    "Hier sollte \"length\" erscheinen - das passiert aber nicht immer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.choices[0].finish_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geschwindigkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Kennst du ein Donauwellenrezept?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "duration = datetime.now() - start\n",
    "\n",
    "# Dauer in sekunden , Anzahl der generierten Tokens\n",
    "duration.total_seconds(), completion.usage.completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens per second\n",
    "completion.usage.completion_tokens / duration.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-4-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "duration = datetime.now() - start\n",
    "\n",
    "# Dauer in sekunden , Anzahl der generierten Tokens\n",
    "duration.total_seconds(), completion.usage.completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens per second\n",
    "completion.usage.completion_tokens / duration.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    stream=True    \n",
    "    )\n",
    "\n",
    "# create variables to collect the stream of chunks\n",
    "collected_chunks = []\n",
    "collected_messages = []\n",
    "# iterate through the stream of events\n",
    "for chunk in response:\n",
    "    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n",
    "    collected_chunks.append(chunk)  # save the event response\n",
    "    chunk_message = chunk.choices[0].delta.content  # extract the message\n",
    "    collected_messages.append(chunk_message)  # save the message\n",
    "    print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n",
    "\n",
    "# print the time delay and text received\n",
    "print(f\"Full response received {chunk_time:.2f} seconds after request\")\n",
    "# clean None in collected_messages\n",
    "collected_messages = [m for m in collected_messages if m is not None]\n",
    "full_reply_content = ''.join([m for m in collected_messages])\n",
    "print(f\"Full conversation received: {full_reply_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model input / output (Huggingface / OpenSource Modelle)\n",
    "\n",
    "![llm.png](images/llm.png)\n",
    "\n",
    "### Wie kommen wir an den \"neuen\" Token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface Modell laden\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float32, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt definieren + tokenizen\n",
    "inputs = tokenizer('''Elvis Aaron Presley (* January 8, 1935 in''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wir geben die inputs in das Modell - die outputs verwenden wir um den neuen Token zu erhalten\n",
    "outputs = model(**inputs)\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wir suchen den letzten Output des Modells und bekommen die predictions / Wahrscheinlichkeiten für den neuen Token\n",
    "new_token_logits = outputs.logits[:, -1, :]\n",
    "new_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mit .argmax() suchen wir den Token mit der höchsten Wahrscheinlichkeit -> das wird unser neuer Token\n",
    "new_token = new_token_logits.argmax(dim=1)\n",
    "tokenizer.decode(new_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generieren mit Huggingface\n",
    "\n",
    "Huggingface bietet uns dafür die .generate() Funktion - wir müssen das alles nicht manuell machen :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_length=10)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generieren ist eine \"for loop\"\n",
    "\n",
    "wir können das aber auch manuell - Text generieren mit LLMs ist eine \"for-loop\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('''Elvis Aaron Presley (* January 8, 1935 in''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "for i in range(10):\n",
    "    # Neuen Token generieren\n",
    "    outputs = model(**inputs)\n",
    "    new_token_logits = outputs.logits[:, -1, :]\n",
    "    new_token = new_token_logits.argmax(dim=1)\n",
    "\n",
    "    # Anhängen des neuen Tokens an die Inputs\n",
    "    inputs[\"input_ids\"] = torch.cat((inputs[\"input_ids\"], new_token.unsqueeze(-1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
