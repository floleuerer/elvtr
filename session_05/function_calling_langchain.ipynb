{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pydantic langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI KEY lesen\n",
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_KEY = userdata.get('OPENAI_KEY')\n",
    "except:\n",
    "    OPENAI_KEY = os.getenv('OPENAI_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import Any, Type\n",
    "from langchain_core.tools import BaseTool\n",
    "import json\n",
    "import requests\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Function-Calling\"\n",
    "\n",
    "Damit wir eine Funktion aufrufen können, müssen wir diese definieren. Wichtig dabei ist:\n",
    "- Type-Annotations: wir müssen die Typen der In- und Output Variablen definieren\n",
    "- Wir müssen die Funktion per Docstring beschreiben - mit dieser Beschreibung helfen wir dem LLM zu erkennen, wofür die Funktion verwendet werden kann "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain bietet uns mit \"convert_to_openai_tool\" einen Helfer, der die Funktionsdefinition automatisch erzeugt. Wir müssen diese also nicht mehr kompliziert selbst schreiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(convert_to_openai_tool(multiply), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können die funktion dann dem LLM \"bekannt\" machen und einen Prompt abschicken der die Funktion benötigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [convert_to_openai_tool(multiply)]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "llm_with_tools.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain bietet uns noch einen weiteres Tool, nämlich den `JsonOutputToolsParser` - damit wird die aufzurufende Funktion und die Parameter extrahiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_chain = llm_with_tools | JsonOutputToolsParser()\n",
    "result = tool_chain.invoke(\"what's 3 * 12\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir die Funktion aufrufen und das Ergebnis der Multiplikation sehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply(**result[0]['args'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs aufrufen mit \"Function Calling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, FunctionMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_search(keyword: str) -> str:\n",
    "    \"\"\"Search product that contains a keyword\n",
    "    returns a json string containing a list of products with product details\n",
    "\n",
    "    Args:\n",
    "        keyword: Keyword to search for\n",
    "    \"\"\"\n",
    "    response = requests.get(f'https://dummyjson.com/products/search?q={keyword}')\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([convert_to_openai_tool(product_search)])\n",
    "tool_chain = llm_with_tools | JsonOutputToolsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Ich brauche ein neues iPhone, welches hat die beste Bewertung?\")]\n",
    "result = tool_chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = product_search(**result[0]['args'])\n",
    "json.loads(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(FunctionMessage(name='product_search', content=search_result))\n",
    "result = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
