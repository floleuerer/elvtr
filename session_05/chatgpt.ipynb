{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai langchain langchain-openai gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI KEY lesen\n",
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_KEY = userdata.get('OPENAI_KEY')\n",
    "except:\n",
    "    OPENAI_KEY = os.getenv('OPENAI_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import openai\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 1: LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-0613')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(content='Du bist ein hilfsbereiter Assistent!')\n",
    "prompt = HumanMessage(content='Kennst du ein Donauwellenrezept?')\n",
    "result = llm([system_message, prompt])\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2: LLM + History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(content='Du bist ein hilfsbereiter Assistent!')\n",
    "history = [system_message]\n",
    "def chat(prompt):\n",
    "    history.append(HumanMessage(content=prompt))\n",
    "    result = llm(history)\n",
    "    history.append(AIMessage(content=result.content))\n",
    "    return result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chat('Kennst du ein Donauwellenrezept?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chat('Gibts das auch vegan?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 3: LLM + History + History Management\n",
    "\n",
    "Wir müssen die Länge der Chat History begrenzen. Ansonsten kann es passieren, dass wir bei langen Konversationen die Kontext-Länge des Modells überschreiten. Außerdem könenn wir so die Kosten im Griff behalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens mit langchain zählen\n",
    "llm.get_num_tokens_from_messages(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_history(history, max_history_tokens=500):\n",
    "    while llm.get_num_tokens_from_messages(history) > max_history_tokens:\n",
    "        # so lange unsere history größer als max_tokens ist löschen wir die zweite nachricht. Damit stellen wir sicher, dass immer die system_message erhalten bleibt!\n",
    "        del history[1]\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cut_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.get_num_tokens_from_messages(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frage 1\n",
    "\n",
    "Was hat das abschneiden der ältesten Nachrichten in der Chat-History zur Folge?\n",
    "\n",
    "#### Frage 2\n",
    "\n",
    "Gibt es eine Alternative zum Abschneiden der ältesten Nachrichten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 4: LLM + History + History Management + Benutzeroberfläche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(content='Du bist ein hilfsbereiter Assistent!')\n",
    "history = [system_message]\n",
    "\n",
    "def chat(prompt):\n",
    "    history.append(HumanMessage(content=prompt))\n",
    "    result = llm(history)\n",
    "    history.append(AIMessage(content=result.content))\n",
    "    return result.content\n",
    "\n",
    "def cut_history(history, max_history_tokens=500):\n",
    "    while llm.get_num_tokens_from_messages(history) > max_history_tokens:\n",
    "        # so lange unsere history größer als max_tokens ist löschen wir die zweite nachricht. Damit stellen wir sicher, dass immer die system_message erhalten bleibt!\n",
    "        del history[1]\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(message, history):\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "    gpt_response = llm(history_langchain_format)\n",
    "    return gpt_response.content\n",
    "\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
