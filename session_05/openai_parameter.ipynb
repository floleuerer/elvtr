{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI KEY lesen\n",
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_KEY = userdata.get('OPENAI_KEY')\n",
    "except:\n",
    "    OPENAI_KEY = os.getenv('OPENAI_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI DeepDive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_tokens\n",
    "\n",
    "Wie viele Tokens soll das Modell ausgeben?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  max_tokens=10,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Du bist ein poetischer Assistent, der komplexe Generative AI-Konzepte mit kreativem Gespür erklären kann.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Verfasse ein Gedicht, das das Konzept des Trainings großer Sprachmodelle erklärt\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature = 0 (wenig Halluzinationen)\n",
    "\n",
    "Temperature 0 bedeutet, dass die Antworten (fast) deterministisch sind. D.h. ein Prompt ergibt immer dieselbe Antwort. Die Temperature sollte 0 sein, wenn man Halluzinationen vermeiden möchte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  temperature=0,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Du bist ein poetischer Assistent, der komplexe Generative AI-Konzepte mit kreativem Gespür erklären kann.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Verfasse ein Gedicht, das das Konzept des Trainings großer Sprachmodelle erklärt\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  temperature=0,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Du bist ein poetischer Assistent, der komplexe Generative AI-Konzepte mit kreativem Gespür erklären kann.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Verfasse ein Gedicht, das das Konzept des Trainings großer Sprachmodelle erklärt\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature = 1 (> 0 - Antworten sind \"kreativer\" bzw zufälliger) \n",
    "\n",
    "Temperature größer 0 bedeutet, dass derselbe Prompt verschieden Antworten ergibt. Man sagt, die Antworten sind \"kreativer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  temperature=1.0,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Du bist ein poetischer Assistent, der komplexe Generative AI-Konzepte mit kreativem Gespür erklären kann.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Verfasse ein Gedicht, das das Konzept des Trainings großer Sprachmodelle erklärt\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  temperature=1.0,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Du bist ein poetischer Assistent, der komplexe Generative AI-Konzepte mit kreativem Gespür erklären kann.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Verfasse ein Gedicht, das das Konzept des Trainings großer Sprachmodelle erklärt\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature zu hoch (ab ca 1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  temperature=1.8,\n",
    "  max_tokens=200,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Du bist ein poetischer Assistent, der komplexe Generative AI-Konzepte mit kreativem Gespür erklären kann.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Verfasse ein Gedicht, das das Konzept des Trainings großer Sprachmodelle erklärt\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logit_bias\n",
    "\n",
    "Mit `logit_bias` können wir die Wahrscheinlichkeiten der predicteten Tokens manuell ändern. Wir könne das z. B. nutzen um bestimmte Tokens zu verbieten.\n",
    "\n",
    "Beispiel: wir wollen verhindern, dass \"once upon a\" mit \"time ...\" vervollständigt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create( \n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=50, \n",
    "    messages=[{\"role\": \"system\", \"content\": \"You finish user's sentences.\"},\n",
    "              {\"role\": \"user\", \"content\": \"Once upon a\"}],\n",
    "\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... wir unterdrücken \"time\" mit dem Parameter `logit_bias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wir suchen verschiedene \"Time\" tokens\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "encoding.encode(' time'), encoding.encode('time'), encoding.encode(' Time'), encoding.encode('Time'), encoding.encode('.time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create( \n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=[{\"role\": \"system\", \"content\": \"You finish user's sentences.\"},\n",
    "              {\"role\": \"user\", \"content\": \"Once upon a\"}],\n",
    "    # und fügen diese also logit_bias mit dem wert -100 hinzu\n",
    "    logit_bias={1712:-100, 892:-100, 4212: -100, 1489:-100, 6512:-100}\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d4t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
